from collections import defaultdict
import nltk
from nltk.corpus import gutenberg
from nltk.tokenize import word_tokenize
from nltk.metrics import edit_distance
from tabulate import tabulate

# Error Model
error_model = {}

with open('count_1edit.txt', 'r') as file:
    for line in file:
        parts = line.strip().split(' ')
        if len(parts) == 2:
            wrong_right, count = parts
            if '|' in wrong_right:
                wrong, right = wrong_right.split('|')
                if wrong and right:
                    if wrong not in error_model:
                        error_model[wrong] = {}
                    error_model[wrong][right] = int(count)
                else:
                    print(f"Issue with line: {line.strip()}")
            else:
                print(f"Issue with line: {line.strip()}")
        else:
            print(f"Issue with line: {line.strip()}")

# Language Model
nltk.download('gutenberg')
corpus = gutenberg.words()
vocab = set(corpus)
word_count = len(corpus)
word_frequency = nltk.FreqDist(corpus)

# Spell Check Function
def spell_check(word):
    if word in vocab:
        return f"No errors found in '{word}'"

    candidates = {}
    for w in vocab:
        if abs(len(word) - len(w)) > 1:
            continue

        if edit_distance(word, w) == 1:
            candidates[w] = edit_distance(word, w)

    if not candidates:
        return f"No suggestions found for '{word}'"

    output_data = []
    for candidate, edit_type in candidates.items():
        # Error Model Probability: P(c)
        p_c = 1  # Placeholder for P(c) probability

        # Probability of generating the user-input word from the candidate: P(w|c)
        p_w_given_c = 1.0 / (word_count + 1)  # Laplace smoothing; initialize to avoid zero probability

        if word_frequency[candidate] > 0:
            p_w_given_c = word_frequency[candidate] / word_count

        # Combining P(c) and P(w|c) for each candidate
        combined_probability = p_c * p_w_given_c

        output_data.append([word, candidate, edit_type, f'...', p_c, p_w_given_c, combined_probability])

    # Sort suggestions by combined probability in descending order
    output_data.sort(key=lambda x: x[6], reverse=True)

    # Return the suggestion with the highest combined probability
    best_suggestion = output_data[0]
    return tabulate(output_data, headers=["word", "candidate", "edit_type", "edit", "P(c)", "P(w|c)", "P(c) x P(w|c)"], tablefmt="pretty")

# Example
input_word = "mohter"
result = spell_check(input_word)
print(result)
